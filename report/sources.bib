@inproceedings{LibAFL,
  author    = {Fioraldi, Andrea and Maier, Dominik Christian and Zhang, Dongjia and Balzarotti, Davide},
  title     = {{LibAFL: A Framework to Build Modular and Reusable Fuzzers}},
  year      = {2022},
  isbn      = {9781450394505},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3548606.3560602},
  doi       = {10.1145/3548606.3560602},
  abstract  = {The release of AFL marked an important milestone in the area of software security testing, revitalizing fuzzing as a major research topic and spurring a large number of research studies that attempted to improve and evaluate the different aspects of the fuzzing pipeline.Many of these studies implemented their techniques by forking the AFL codebase. While this choice might seem appropriate at first, combining multiple forks into a single fuzzer requires a high engineering overhead, which hinders progress in the area and prevents fair and objective evaluations of different techniques. The highly fragmented landscape of the fuzzing ecosystem also prevents researchers from combining orthogonal techniques and makes it difficult for end users to adopt new prototype solutions.To tackle this problem, in this paper we propose LibAFL, a framework to build modular and reusable fuzzers. We discuss the different components generally used in fuzzing and map them to an extensible framework. LibAFL allows researchers and engineers to extend the core fuzzer pipeline and share their new components for further evaluations. As part of LibAFL, we integrated techniques from more than 20 previous works and conduct extensive experiments to show the benefit of our framework to combine and evaluate different approaches. We hope this can help to shed light on current advancements in fuzzing and provide a solid base for comparative and extensible research in the future.},
  booktitle = {{Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security}},
  pages     = {1051–1065},
  numpages  = {15},
  keywords  = {framework, fuzz testing, fuzzing},
  location  = {Los Angeles, CA, USA},
  series    = {CCS '22}
}

@online{LibAFLBook,
  title   = {{The LibAFL Fuzzing Library}},
  year    = {n.d.},
  author  = {Fioraldi, Andrea and Maier, Dominik},
  url     = {https://aflplus.plus/libafl-book/},
  urldate = {2024-05-28}
}

@online{AFL,
  title   = {{Fuzzing}},
  author  = {Moroz, Max},
  date    = {2021-06-08},
  url     = {https://github.com/google/AFL},
  urldate = {2024-05-21}
}

@online{AFLCoreutils,
  title   = {{Fuzzing}},
  author  = {Sjöbom, Anders and Hasselberg, Adam},
  date    = {2019-04-25},
  url     = {https://github.com/adamhass/fuzzing/},
  urldate = {2024-05-21}
}

@misc{FileUtilsAnnouncement,
  author       = {MacKenzie, David J. },
  title        = {{GNU file utilities release 1.0}},
  howpublished = {Email to gnu.utils.bug Email List},
  date         = {1990-02-08},
  url          = {https://groups.google.com/g/gnu.utils.bug/c/CviP42X_hCY/m/YssXFn-JrX4J},
  urldate      = {2024-05-22}
}

@misc{TextUtilsAnnouncement,
  author       = {MacKenzie, David J. },
  title        = {{new GNU file and text utilities released}},
  howpublished = {Email to gnu.utils.bug Email List},
  date         = {1991-07-15},
  url          = {https://groups.google.com/g/gnu.utils.bug/c/iN5KuoJYRhU/m/V_6oiBAWF0EJ},
  urldate      = {2024-05-22}
}

@misc{ShellUtilsAnnouncement,
  author       = {MacKenzie, David J. },
  title        = {{GNU shell programming utilities released}},
  howpublished = {Email to gnu.utils.bug Email List},
  date         = {1991-08-22},
  url          = {https://groups.google.com/g/gnu.utils.bug/c/xpTRtuFpNQc/m/mRc_7JWZ0BYJ},
  urldate      = {2024-05-22}
}

@misc{CoreUtilsAnnouncement,
  author       = {Meyering, Jim},
  title        = {{package renamed to coreutils}},
  howpublished = {git commit},
  date         = {2003-01-13},
  url          = {https://git.savannah.gnu.org/cgit/coreutils.git/tree/README-package-renamed-to-coreutils},
  urldate      = {2024-05-22}
}

@online{CoreUtilsHomepage,
  title   = {{Coreutils - GNU core utilities}},
  author  = {Meyering, Jim and Brady, Pádraig and Voelker, Bernhard and Blake, Eric and Eggert, Paul and Gordon, Assaf},
  date    = {2020-12-23},
  url     = {https://www.gnu.org/software/coreutils/coreutils.html},
  urldate = {2024-05-22}
}

@online{Autoconf,
  title   = {{Autoconf}},
  author  = {Meyering, Jim},
  date    = {2020-12-08},
  url     = {https://www.gnu.org/software/autoconf},
  urldate = {2024-05-24}
}
@online{Automake,
  title   = {{Automake}},
  author  = {Meyering, Jim},
  date    = {2022-01-31},
  url     = {https://www.gnu.org/software/automake},
  urldate = {2024-05-24}
}

@online{GNULinux,
  title   = {{Linux and the GNU System}},
  author  = {Stallman, Richard},
  date    = {2021-11-02},
  url     = {https://www.gnu.org/gnu/linux-and-gnu.en.html},
  urldate = {2024-05-22}
}

@misc{GNUCoreUtils9.5,
  title        = {{GNU coreutils 9.5}},
  author       = {Meyering, Jim and Brady, Pádraig and Voelker, Bernhard and Blake, Eric and Eggert, Paul and Gordon, Assaf},
  url          = {https://ftp.gnu.org/gnu/coreutils/coreutils-9.5.tar.gz},
  howpublished = {Software Release},
  date         = {2024-03-28},
  urldate      = {2024-05-22}
}

@online{BusyBox,
  title   = {{BusyBox: The Swiss Army Knife of Embedded Linux}},
  year    = {n.d.},
  url     = {https://www.busybox.net/about.html},
  urldate = {2024-05-22}
}

@online{Alpine,
  title   = {{Alpine Linux — About}},
  year    = {n.d.},
  url     = {https://alpinelinux.org/about/},
  urldate = {2024-05-22}
}

@online{Uutils,
  title   = {{uutils}},
  year    = {n.d.},
  url     = {https://uutils.github.io/},
  urldate = {2024-05-22}
}

@online{UutilsCoreUtils,
  title   = {{uutils — coreutils}},
  year    = {n.d.},
  url     = {https://uutils.github.io/coreutils},
  urldate = {2024-05-22}
}

@online{SanitizerCoverage,
  title   = {{SanitizerCoverage}},
  year    = {n.d.},
  url     = {https://clang.llvm.org/docs/SanitizerCoverage.html},
  urldate = {2024-05-28}
}

@article{UNIX,
  author     = {Miller, Barton P. and Fredriksen, Lars and So, Bryan},
  title      = {{An Empirical Study of the Reliability of UNIX Utilities}},
  year       = {1990},
  issue_date = {Dec. 1990},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {33},
  number     = {12},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/96267.96279},
  doi        = {10.1145/96267.96279},
  abstract   = {The following section describes the tools we built to test the utilities. These tools include the fuzz (random character) generator, ptyjig (to test interactive utilities), and scripts to automate the testing process. Next, we will describe the tests we performed, giving the types of input we presented to the utilities. Results from the tests will follow along with an analysis of the results, including identification and classification of the program bugs that caused the crashes. The final section presents concluding remarks, including suggestions for avoiding the types of problems detected by our study and some commentary on the bugs we found. We include an Appendix with the user manual pages for fuzz and ptyjig.},
  journal    = {Commun. ACM},
  month      = {12},
  pages      = {32–44},
  numpages   = {13}
}

@unpublished{EVA,
  author = {Valentin Huber},
  title  = {{Challenges and Mitigation Strategies in Symbolic Execution Based Fuzzing Through the Lens of Survey Papers}},
  year   = {2023},
  month  = {12},
  day    = {15},
  url    = {https://github.com/riesentoaster/review-symbolic-execution-in-fuzzing/releases/download/v1.0/Huber-Valentin-Challenges-and-Mitigation-Strategies-in-Symbolic-Execution-Based-Fuzzing-Through-the-Lens-of-Survey-Papers.pdf}
}

@unpublished{VT1,
  author = {Valentin Huber},
  title  = {{Running KLEE on GNU coreutils}},
  year   = {2024},
  month  = {02},
  day    = {13},
  url    = {https://github.com/riesentoaster/klee-coreutils-experiments/releases/download/v1.0/Huber-Valentin-running-KLEE-on-coreutils-report.pdf}
}

@article{AFLFast,
  author   = {Böhme, Marcel and Pham, Van-Thuan and Roychoudhury, Abhik},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {Coverage-Based Greybox Fuzzing as Markov Chain},
  year     = {2019},
  volume   = {45},
  number   = {5},
  pages    = {489-506},
  keywords = {Schedules;Markov processes;Computer crashes;Search problems;Tools;Systematics;Vulnerability detection;fuzzing;path exploration;symbolic execution;automated testing},
  doi      = {10.1109/TSE.2017.2785841}
}

@inproceedings{MOPT,
  author    = {Chenyang Lyu and Shouling Ji and Chao Zhang and Yuwei Li and Wei-Han Lee and Yu Song and Raheem Beyah},
  title     = {{MOPT}: Optimized Mutation Scheduling for Fuzzers},
  booktitle = {28th USENIX Security Symposium (USENIX Security 19)},
  year      = {2019},
  isbn      = {978-1-939133-06-9},
  address   = {Santa Clara, CA},
  pages     = {1949--1966},
  url       = {https://www.usenix.org/conference/usenixsecurity19/presentation/lyu},
  publisher = {USENIX Association},
  month     = aug
}

@article{Demystifying,
  author     = {Mallissery, Sanoop and Wu, Yu-Sung},
  title      = {Demystify the Fuzzing Methods: A Comprehensive Survey},
  year       = {2023},
  issue_date = {March 2024},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {56},
  number     = {3},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3623375},
  doi        = {10.1145/3623375},
  abstract   = {Massive software applications possess complex data structures or parse complex data structures; in such cases, vulnerabilities in the software become inevitable. The vulnerabilities are the source of cyber-security threats, and discovering this before the software deployment is challenging. Fuzzing is a vulnerability discovery solution that resonates with random-mutation, feedback-driven, coverage-guided, constraint-guided, seed-scheduling, and target-oriented strategies. Each technique is wrapped beneath the black-, white-, and grey-box fuzzers to uncover diverse vulnerabilities. It consists of methods such as identifying structural information about the test cases to detect security vulnerabilities, symbolic and concrete program states to explore the unexplored locations, and full semantics of code coverage to create new test cases. We methodically examine each kind of fuzzers and contemporary fuzzers with a profound observation that addresses various research questions and systematically reviews and analyze the gaps and their solutions. Our survey comprised the recent related works on fuzzing techniques to demystify the fuzzing methods concerning the application domains and the target that, in turn, achieves higher code coverage and sound vulnerability detection.},
  journal    = {ACM Comput. Surv.},
  month      = {10},
  articleno  = {71},
  numpages   = {38},
  keywords   = {vulnerability discovery, Automated testing, fuzzing, code inspection}
}

@article{SurveyRoadmap,
  author     = {Zhu, Xiaogang and Wen, Sheng and Camtepe, Seyit and Xiang, Yang},
  title      = {Fuzzing: A Survey for Roadmap},
  year       = {2022},
  issue_date = {January 2022},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {54},
  number     = {11s},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3512345},
  doi        = {10.1145/3512345},
  abstract   = {Fuzz testing (fuzzing) has witnessed its prosperity in detecting security flaws recently. It generates a large number of test cases and monitors the executions for defects. Fuzzing has detected thousands of bugs and vulnerabilities in various applications. Although effective, there lacks systematic analysis of gaps faced by fuzzing. As a technique of defect detection, fuzzing is required to narrow down the gaps between the entire input space and the defect space. Without limitation on the generated inputs, the input space is infinite. However, defects are sparse in an application, which indicates that the defect space is much smaller than the entire input space. Besides, because fuzzing generates numerous test cases to repeatedly examine targets, it requires fuzzing to perform in an automatic manner. Due to the complexity of applications and defects, it is challenging to automatize the execution of diverse applications. In this article, we systematically review and analyze the gaps as well as their solutions, considering both breadth and depth. This survey can be a roadmap for both beginners and advanced developers to better understand fuzzing.},
  journal    = {ACM Comput. Surv.},
  month      = {09},
  articleno  = {230},
  numpages   = {36},
  keywords   = {automation, input space, fuzzing theory, security, Fuzz testing}
}

@online{AFLBugs,
  title   = {{The bug-o-rama trophy case}},
  year    = {n.d.},
  author  = {Zalewski, Michal},
  url     = {https://lcamtuf.coredump.cx/afl/#bugs},
  urldate = {2024-06-03}
}

@online{AFLPlusPlus,
  title   = {{AFL++}},
  year    = {n.d.},
  url     = {https://aflplus.plus},
  urldate = {2024-06-03}
}

@article{UNIXRevisited,
  author   = {Miller, Barton P. and Zhang, Mengxiao and Heymann, Elisa R.},
  journal  = {IEEE Transactions on Software Engineering},
  title    = {The Relevance of Classic Fuzz Testing: Have We Solved This One?},
  year     = {2022},
  volume   = {48},
  number   = {6},
  pages    = {2028-2039},
  keywords = {Tools;Testing;Software;Operating systems;Fuzzing;Software reliability;Linux;Testing and debugging;testing tools},
  doi      = {10.1109/TSE.2020.3047766}
}

@inproceedings{CLIFuzzer,
  author    = {Gupta, Abhilash and Gopinath, Rahul and Zeller, Andreas},
  title     = {{CLIFuzzer}: mining grammars for command-line invocations},
  year      = {2022},
  isbn      = {9781450394130},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3540250.3558918},
  doi       = {10.1145/3540250.3558918},
  abstract  = {The behavior of command-line utilities can be very much influenced by passing command-line options and arguments—configuration 
               settings that enable, disable, or otherwise influence parts of the code to be executed. Hence, systematic testing of command-line utilities requires testing them with diverse configurations of supported command-line options. 
               
               We introduce CLIFuzzer, a tool that takes an executable program and, using dynamic analysis to track input processing, automatically 
               extract a full set of its options, arguments, and argument types. This set forms a grammar that represents the valid sequences of valid 
               options and arguments. Producing invocations from this grammar, we can fuzz the program with an endless list of random configurations, covering the related code. This leads to increased coverage and new bugs over purely mutation based fuzzers.},
  booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages     = {1667–1671},
  numpages  = {5},
  keywords  = {CLI Options, command-line, fuzzing, utilities},
  location  = {<conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>},
  series    = {ESEC/FSE 2022}
}

@online{AFLPlusPlusargv,
  title   = {{Struggling to give inputs to AFL}},
  year    = {n.d.},
  author  = {Zalewski, Michal},
  url     = {https://groups.google.com/g/afl-users/c/ZBWq0LdHBzw/m/zBlo7q9LBAAJ},
  urldate = {2024-06-04}
}
@inproceedings{SEDiff,
  author    = {Li, Penghui and Meng, Wei and Lu, Kangjie},
  title     = {{SEDiff}: scope-aware differential fuzzing to test internal function models in symbolic execution},
  year      = {2022},
  isbn      = {9781450394130},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3540250.3549080},
  doi       = {10.1145/3540250.3549080},
  abstract  = {Symbolic execution has become a foundational program analysis technique.  
               Performing symbolic execution unavoidably encounters internal functions (e.g., library functions) that provide basic operations such as string processing.  
               Many symbolic execution engines construct internal function models that abstract function behaviors for scalability and compatibility concerns.  
               Due to the high complexity of constructing the models,  
               developers intentionally summarize only partial behaviors of a function, namely modeled functionalities, in the models.  
               The correctness of the internal function models is critical because  
               it would impact all applications of symbolic execution, e.g., bug detection and model checking.  
               
               A naive solution to testing the correctness of internal function models is to cross-check whether the behaviors of the models comply with their corresponding original function implementations.  
               However, such a solution would mostly detect overwhelming inconsistencies concerning the unmodeled functionalities, which are out of the scope of models and thus considered false reports.  
               We argue that a reasonable testing approach should target only the functionalities that developers intend to model.  
               While being necessary, automatically identifying the modeled functionalities, i.e., the scope, is a significant challenge.  
               
               In this paper, we propose a scope-aware differential testing framework, SEDiff, to tackle this problem.  
               We design a novel algorithm to automatically map the modeled functionalities to the code in the original implementations.  
               SEDiff then applies scope-aware grey-box differential fuzzing to relevant code in the original implementations.  
               It also equips a new scope-aware input generator and a tailored bug checker that efficiently and correctly detect erroneous inconsistencies.  
               We extensively evaluated SEDiff on several popular real-world symbolic execution engines targeting binary, web and kernel.  
               Our manual investigation shows that SEDiff precisely identifies the modeled functionalities and detects  
               46 new bugs in the internal function models used in the symbolic execution engines.},
  booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages     = {57–69},
  numpages  = {13},
  keywords  = {Symbolic Execution, Internal Function Models, Differential Testing},
  location  = {<conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>},
  series    = {ESEC/FSE 2022}
}

@mastersthesis{DarpaExtended,
  author = {Kilmer, Eric David},
  school = {Penn State University},
  title  = {Extending Vulnerability Discovery with Fuzzing and Symbolic Execution to Realistic Applications},
  date   = {2017-06-09}
}
@conference{Seccomp,
  author       = {Marcus Gelderie. and Valentin Barth. and Maximilian Luff. and Julian Birami.},
  title        = {Seccomp Filters from Fuzzing},
  booktitle    = {Proceedings of the 19th International Conference on Security and Cryptography - SECRYPT},
  year         = {2022},
  pages        = {507-512},
  publisher    = {SciTePress},
  organization = {INSTICC},
  doi          = {10.5220/0011145100003283},
  isbn         = {978-989-758-590-6},
  issn         = {2184-7711}
}

@inproceedings{FUSE,
  author    = {Zhang, Guofeng and Chen, Zhenbang and Shuai, Ziqi and Zhang, Yufeng and Wang, Ji},
  booktitle = {2022 29th Asia-Pacific Software Engineering Conference (APSEC)},
  title     = {Synergizing Symbolic Execution and Fuzzing By Function-level Selective Symbolization},
  year      = {2022},
  volume    = {},
  number    = {},
  pages     = {328-337},
  keywords  = {Codes;Fuses;Source coding;Semantics;Fuzzing;Benchmark testing;Libraries;Symbolic Execution;Constraint Solving;Fuzzing;Environment Modeling},
  doi       = {10.1109/APSEC57359.2022.00045}
}

@inproceedings{KLEE,
  author    = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson},
  title     = {{KLEE}: unassisted and automatic generation of high-coverage tests for complex systems programs},
  year      = {2008},
  publisher = {USENIX Association},
  address   = {USA},
  abstract  = {We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage -- on average over 90\% per tool (median: over 94\%) -- and significantly beat the coverage of the developers' own hand-written test suite. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100\% coverage on 31 of them.We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to crosscheck purportedly identical BUSYBOX and COREUTILS utilities, finding functional correctness errors and a myriad of inconsistencies.},
  booktitle = {Proceedings of the 8th USENIX Conference on Operating Systems Design and Implementation},
  pages     = {209–224},
  numpages  = {16},
  location  = {San Diego, California},
  series    = {OSDI'08}
}

@inproceedings{CRETE,
  author    = {Chen, Bo and Havlicek, Christopher and Yang, Zhenkun and Cong, Kai and Kannavara, Raghudeep and Xie, Fei},
  editor    = {Russo, Alessandra and Sch{\"u}rr, Andy},
  title     = {{CRETE}: A Versatile Binary-Level Concolic Testing Framework},
  booktitle = {Fundamental Approaches to Software Engineering},
  year      = {2018},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {281--298},
  abstract  = {In this paper, we present crete, a versatile binary-level concolic testing framework, which features an open and highly extensible architecture allowing easy integration of concrete execution frontends and symbolic execution engine backends. crete's extensibility is rooted in its modular design where concrete and symbolic execution is loosely coupled only through standardized execution traces and test cases. The standardized execution traces are llvm-based, self-contained, and composable, providing succinct and sufficient information for symbolic execution engines to reproduce the concrete executions. We have implemented crete with klee as the symbolic execution engine and multiple concrete execution frontends such as qemu and 8051 Emulator. We have evaluated the effectiveness of crete on GNU Coreutils programs and TianoCore utility programs for UEFI BIOS. The evaluation of Coreutils programs shows that crete achieved comparable code coverage as klee directly analyzing the source code of Coreutils and generally outperformed angr. The evaluation of TianoCore utility programs found numerous exploitable bugs that were previously unreported.},
  isbn      = {978-3-319-89363-1}
}

@inproceedings{IMF-SIM,
  author    = {Wang, Shuai and Wu, Dinghao},
  booktitle = {2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  title     = {In-memory fuzzing for binary code similarity analysis},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {319-330},
  keywords  = {Binary codes;Tools;Runtime;Indexes;Syntactics;In-memory fuzzing;code similarity;reverse engineering;taint analysis},
  doi       = {10.1109/ASE.2017.8115645}
}

@inproceedings{Learch,
  author    = {He, Jingxuan and Sivanrupan, Gishor and Tsankov, Petar and Vechev, Martin},
  title     = {Learning to Explore Paths for Symbolic Execution},
  year      = {2021},
  isbn      = {9781450384544},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3460120.3484813},
  doi       = {10.1145/3460120.3484813},
  abstract  = {Symbolic execution is a powerful technique that can generate tests steering program execution into desired paths. However, the scalability of symbolic execution is often limited by path explosion, i.e., the number of symbolic states representing the paths under exploration quickly explodes as execution goes on. Therefore, the effectiveness of symbolic execution engines hinges on the ability to select and explore the right symbolic states.In this work, we propose a novel learning-based strategy, called Learch, able to effectively select promising states for symbolic execution to tackle the path explosion problem. Learch directly estimates the contribution of each state towards the goal of maximizing coverage within a time budget, as opposed to relying on manually crafted heuristics based on simple statistics as a crude proxy for the objective. Moreover, Learch leverages existing heuristics in training data generation and feature extraction, and can thus benefit from any new expert-designed heuristics. We instantiated Learch in KLEE, a widely adopted symbolic execution engine. We evaluated Learch on a diverse set of programs, showing that Learch is practically effective: it covers more code and detects more security violations than existing manual heuristics, as well as combinations of those heuristics. We also show that using tests generated by Learch as initial fuzzing seeds enables the popular fuzzer AFL to find more paths and security violations.},
  booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {2526–2540},
  numpages  = {15},
  keywords  = {fuzzing, machine learning, program testing, symbolic execution},
  location  = {Virtual Event, Republic of Korea},
  series    = {CCS '21}
}

@inproceedings{Cloud9,
  author    = {Kuznetsov, Volodymyr and Kinder, Johannes and Bucur, Stefan and Candea, George},
  title     = {Efficient state merging in symbolic execution},
  year      = {2012},
  isbn      = {9781450312059},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2254064.2254088},
  doi       = {10.1145/2254064.2254088},
  abstract  = {Symbolic execution has proven to be a practical technique for building automated test case generation and bug finding tools. Nevertheless, due to state explosion, these tools still struggle to achieve scalability. Given a program, one way to reduce the number of states that the tools need to explore is to merge states obtained on different paths. Alas, doing so increases the size of symbolic path conditions (thereby stressing the underlying constraint solver) and interferes with optimizations of the exploration process (also referred to as search strategies). The net effect is that state merging may actually lower performance rather than increase it.We present a way to automatically choose when and how to merge states such that the performance of symbolic execution is significantly increased. First, we present query count estimation, a method for statically estimating the impact that each symbolic variable has on solver queries that follow a potential merge point; states are then merged only when doing so promises to be advantageous. Second, we present dynamic state merging, a technique for merging states that interacts favorably with search strategies in automated test case generation and bug finding tools.Experiments on the 96 GNU Coreutils show that our approach consistently achieves several orders of magnitude speedup over previously published results. Our code and experimental data are publicly available at http://cloud9.epfl.ch.},
  booktitle = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages     = {193–204},
  numpages  = {12},
  keywords  = {verification, testing, symbolic execution, state merging, bounded software model checking},
  location  = {Beijing, China},
  series    = {PLDI '12}
}

@inproceedings{DLFuzz,
  author    = {Guo, Jianmin and Jiang, Yu and Zhao, Yue and Chen, Quan and Sun, Jiaguang},
  title     = {{DLFuzz}: differential fuzzing testing of deep learning systems},
  year      = {2018},
  isbn      = {9781450355735},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3236024.3264835},
  doi       = {10.1145/3236024.3264835},
  abstract  = {Deep learning (DL) systems are increasingly applied to safety-critical domains such as autonomous driving cars. It is of significant importance to ensure the reliability and robustness of DL systems. Existing testing methodologies always fail to include rare inputs in the testing dataset and exhibit low neuron coverage. In this paper, we propose DLFuzz, the first differential fuzzing testing framework to guide DL systems exposing incorrect behaviors. DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality. We present empirical evaluations on two well-known datasets to demonstrate its efficiency. Compared with DeepXplore, the state-of-the-art DL whitebox testing framework, DLFuzz does not require extra efforts to find similar functional DL systems for cross-referencing check, but could generate 338.59\% more adversarial inputs with 89.82\% smaller perturbations, averagely obtain 2.86\% higher neuron coverage, and save 20.11\% time consumption.},
  booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages     = {739–743},
  numpages  = {5},
  keywords  = {Neuron Coverage, Fuzzing Testing, Deep Learning},
  location  = {Lake Buena Vista, FL, USA},
  series    = {ESEC/FSE 2018}
}
@inproceedings{DPIFuzz,
  author    = {Reen, Gaganjeet Singh and Rossow, Christian},
  title     = {{DPIFuzz}: A Differential Fuzzing Framework to Detect DPI Elusion Strategies for QUIC},
  year      = {2020},
  isbn      = {9781450388580},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3427228.3427662},
  doi       = {10.1145/3427228.3427662},
  abstract  = {QUIC is an emerging transport protocol that has the potential to replace TCP in the near future. As such, QUIC will become an important target for Deep Packet Inspection (DPI). Reliable DPI is essential, e.g., for corporate environments, to monitor traffic entering and leaving their networks. However, elusion strategies threaten the validity of DPI systems, as they allow attackers to carefully design traffic to fool and thus evade on-path DPI systems. While such elusion strategies for TCP are well documented, it is unclear if attackers will be able to elude QUIC-based DPI systems. In this paper, we systematically explore elusion methodologies for QUIC. To this end, we present DPIFuzz: a differential fuzzing framework which can automatically detect strategies to elude stateful DPI systems for QUIC. We use DPIFuzz to generate and mutate QUIC streams in order to compare (and find differences in) the server-side interpretations of five popular open-source QUIC implementations. We show that DPIFuzz successfully reveals DPI elusion strategies, such as using packets with duplicate packet numbers or exploiting the diverging handling of overlapping stream offsets by QUIC implementations. DPIFuzz additionally finds four security-critical vulnerabilities in these QUIC implementations.},
  booktitle = {Proceedings of the 36th Annual Computer Security Applications Conference},
  pages     = {332–344},
  numpages  = {13},
  keywords  = {QUIC, Protocol Fuzzing, Differential Fuzzing, DPI Elusion},
  location  = {<conf-loc>, <city>Austin</city>, <country>USA</country>, </conf-loc>},
  series    = {ACSAC '20}
}

@inproceedings{T-Reqs,
  author    = {Jabiyev, Bahruz and Sprecher, Steven and Onarlioglu, Kaan and Kirda, Engin},
  title     = {{T-Reqs}: HTTP Request Smuggling with Differential Fuzzing},
  year      = {2021},
  isbn      = {9781450384544},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3460120.3485384},
  doi       = {10.1145/3460120.3485384},
  abstract  = {HTTP Request Smuggling (HRS) is an attack that exploits the HTTP processing discrepancies between two servers deployed in a proxy-origin configuration, allowing attackers to smuggle hidden requests through the proxy. While this idea is not new, HRS is soaring in popularity due to recently revealed novel exploitation techniques and real-life abuse scenarios. In this work, we step back from the highly-specific exploits hogging the spotlight, and present the first work that systematically explores HRS within a scientific framework. We design an experiment infrastructure powered by a novel grammar-based differential fuzzer, test 10 popular server/proxy/CDN technologies in combinations, identify pairs that result in processing discrepancies, and discover exploits that lead to HRS. Our experiment reveals previously unknown ways to manipulate HTTP requests for exploitation, and for the first time documents the server pairs prone to HRS.},
  booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {1805–1820},
  numpages  = {16},
  keywords  = {HTTP request smuggling, HTTP desync attacks},
  location  = {Virtual Event, Republic of Korea},
  series    = {CCS '21}
}

@article{JVM,
  author     = {Chen, Yuting and Su, Ting and Sun, Chengnian and Su, Zhendong and Zhao, Jianjun},
  title      = {Coverage-directed differential testing of JVM implementations},
  year       = {2016},
  issue_date = {June 2016},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {51},
  number     = {6},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/2980983.2908095},
  doi        = {10.1145/2980983.2908095},
  abstract   = {Java virtual machine (JVM) is a core technology, whose reliability is critical. Testing JVM implementations requires painstaking effort in designing test classfiles (*.class) along with their test oracles. An alternative is to employ binary fuzzing to differentially test JVMs by blindly mutating seeding classfiles and then executing the resulting mutants on different JVM binaries for revealing inconsistent behaviors. However, this blind approach is not cost effective in practice because most of the mutants are invalid and redundant. This paper tackles this challenge by introducing classfuzz, a coverage-directed fuzzing approach that focuses on representative classfiles for differential testing of JVMs’ startup processes. Our core insight is to (1) mutate seeding classfiles using a set of predefined mutation operators (mutators) and employ Markov Chain Monte Carlo (MCMC) sampling to guide mutator selection, and (2) execute the mutants on a reference JVM implementation and use coverage uniqueness as a discipline for accepting representative ones. The accepted classfiles are used as inputs to differentially test different JVM implementations and find defects. We have implemented classfuzz and conducted an extensive evaluation of it against existing fuzz testing algorithms. Our evaluation results show that classfuzz can enhance the ratio of discrepancy-triggering classfiles from 1.7\% to 11.9\%. We have also reported 62 JVM discrepancies, along with the test classfiles, to JVM developers. Many of our reported issues have already been confirmed as JVM defects, and some even match recent clarifications and changes to the Java SE 8 edition of the JVM specification.},
  journal    = {SIGPLAN Not.},
  month      = {06},
  pages      = {85–99},
  numpages   = {15},
  keywords   = {fuzz testing, MCMC sampling, Java virtual machine, Differential testing}
}

@inproceedings{HyDiff,
  author    = {Noller, Yannic and P\u{a}s\u{a}reanu, Corina S. and B\"{o}hme, Marcel and Sun, Youcheng and Nguyen, Hoang Lam and Grunske, Lars},
  title     = {{HyDiff}: hybrid differential software analysis},
  year      = {2020},
  isbn      = {9781450371216},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3377811.3380363},
  doi       = {10.1145/3377811.3380363},
  abstract  = {Detecting regression bugs in software evolution, analyzing side-channels in programs and evaluating robustness in deep neural networks (DNNs) can all be seen as instances of differential software analysis, where the goal is to generate diverging executions of program paths. Two executions are said to be diverging if the observable program behavior differs, e.g., in terms of program output, execution time, or (DNN) classification. The key challenge of differential software analysis is to simultaneously reason about multiple program paths, often across program variants.This paper presents HyDiff, the first hybrid approach for differential software analysis. HyDiff integrates and extends two very successful testing techniques: Feedback-directed greybox fuzzing for efficient program testing and shadow symbolic execution for systematic program exploration. HyDiff extends greybox fuzzing with divergence-driven feedback based on novel cost metrics that also take into account the control flow graph of the program. Furthermore HyDiff extends shadow symbolic execution by applying four-way forking in a systematic exploration and still having the ability to incorporate concrete inputs in the analysis. HyDiff applies divergence revealing heuristics based on resource consumption and control-flow information to efficiently guide the symbolic exploration, which allows its efficient usage beyond regression testing applications. We introduce differential metrics such as output, decision and cost difference, as well as patch distance, to assist the fuzzing and symbolic execution components in maximizing the execution divergence.We implemented our approach on top of the fuzzer AFL and the symbolic execution framework Symbolic PathFinder. Weillustrate HyDiff on regression and side-channel analysis for Java bytecode programs, and further show how to use HyDiff for robustness analysis of neural networks.},
  booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
  pages     = {1273–1285},
  numpages  = {13},
  keywords  = {symbolic execution, fuzzing, differential program analysis},
  location  = {Seoul, South Korea},
  series    = {ICSE '20}
}

@inproceedings{Fluffy,
  author    = {Youngseok Yang and Taesoo Kim and Byung-Gon Chun},
  title     = {Finding Consensus Bugs in Ethereum via Multi-transaction Differential Fuzzing},
  booktitle = {15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21)},
  year      = {2021},
  isbn      = {978-1-939133-22-9},
  pages     = {349--365},
  url       = {https://www.usenix.org/conference/osdi21/presentation/yang},
  publisher = {USENIX Association},
  month     = {07}
}

@inproceedings{JIT-Picking,
  author    = {Bernhard, Lukas and Scharnowski, Tobias and Schloegel, Moritz and Blazytko, Tim and Holz, Thorsten},
  title     = {{JIT-Picking}: Differential Fuzzing of JavaScript Engines},
  year      = {2022},
  isbn      = {9781450394505},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3548606.3560624},
  doi       = {10.1145/3548606.3560624},
  abstract  = {Modern JavaScript engines that power websites and even full applications on the Web are driven by the need for an increasingly fast and snappy user experience. These engines use several complex and potentially error-prone mechanisms to optimize their performance. Unsurprisingly, the inevitable complexity results in a huge attack surface and varioustypes of software vulnerabilities. On the defender's side, fuzz testing has proven to be an invaluable tool for uncovering different kinds of memory safety violations. Although it is difficult to test interpreters and JIT compilers in an automated way, recent proposals for input generation based on grammars or target-specific intermediate representations helped uncovering many software faults. However, subtle logic bugs and miscomputations that arise from optimization passes in JIT engines continue to elude state-of-the-art testing methods. While such flaws might seem unremarkable at first glance, they are often still exploitable in practice. In this paper, we propose a novel technique for effectively uncovering this class of subtle bugs during fuzzing. The key idea is to take advantage of the tight coupling between a JavaScript engine's interpreter and its corresponding JIT compiler as a domain-specific and generic bug oracle, which in turn yields a highly sensitive fault detection mechanism. We have designed and implemented a prototype of the proposed approach in a tool called JIT-Picker. In an empirical evaluation, we show that our method enables us to detect subtle software faults that prior work missed. In total, we uncovered 32 bugs that were not publicly known and received a $10.000 bug bounty from Mozilla as a reward for our contributions to JIT engine security.},
  booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {351–364},
  numpages  = {14},
  keywords  = {software security, jit engine, fuzzing, differential testing, browser},
  location  = {Los Angeles, CA, USA},
  series    = {CCS '22}
}
@inproceedings{DifFuzz,
  author    = {Nilizadeh, Shirin and Noller, Yannic and Pasareanu, Corina S.},
  booktitle = {2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)},
  title     = {{DifFuzz}: Differential Fuzzing for Side-Channel Analysis},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {176-187},
  keywords  = {Fuzzing;Java;Tools;Time factors;Instruments;Correlation;Performance analysis;vulnerability detection;side-channel;dynamic analysis;fuzzing},
  doi       = {10.1109/ICSE.2019.00034}
}

@article{ExploitingDissent,
  author   = {Walz, Andreas and Sikora, Axel},
  journal  = {IEEE Transactions on Dependable and Secure Computing},
  title    = {Exploiting Dissent: Towards Fuzzing-Based Differential Black-Box Testing of TLS Implementations},
  year     = {2020},
  volume   = {17},
  number   = {2},
  pages    = {278-291},
  keywords = {Protocols;Testing;Computer bugs;Cryptography;Servers;Complexity theory;TLS;network security;cryptographic protocols;fuzzing;differential testing},
  doi      = {10.1109/TDSC.2017.2763947}
}

@inproceedings{JVM2,
  author    = {Chen, Yuting and Su, Ting and Su, Zhendong},
  booktitle = {2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)},
  title     = {Deep Differential Testing of JVM Implementations},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {1257-1268},
  keywords  = {Monitoring;Testing;Java;Engines;Security;Computer crashes;Runtime;Differential JVM testing;live bytecode mutation;semantically different mutants},
  doi       = {10.1109/ICSE.2019.00127}
}

@article{EVMFuzz,
  author   = {Fu, Ying and Ren, Meng and Ma, Fuchen and Yang, Xin and Shi, Heyuan and Li, Shanshan and Liao, Xiangke},
  title    = {EVMFuzz: Differential fuzz testing of Ethereum virtual machine},
  journal  = {Journal of Software: Evolution and Process},
  volume   = {36},
  number   = {4},
  pages    = {e2556},
  keywords = {differential testing, domain-specific mutation, EVM, fuzzing},
  doi      = {https://doi.org/10.1002/smr.2556},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2556},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.2556},
  abstract = {Abstract The vulnerabilities in Ethereum virtual machine (EVM) may lead to serious problems for the Ethereum ecosystem. With lots of techniques being developed for the validation of smart contracts, the testing of EVM has not been well-studied. In this paper, we propose EVMFuzz, the first that uses the differential fuzzing technique to detect vulnerabilities in EVM. The core idea of EVMFuzz is to continuously generate seed contracts for different EVMs' execution, so as to find as many inconsistencies among execution results as possible, and eventually discover vulnerabilities with output cross-referencing. First, we present the evaluation metric for the internal inconsistency indicator. Then, we construct seed contracts via predefined mutators and employ a dynamic priority scheduling algorithm to guide seed contract selection and maximize the inconsistency. Finally, we leverage different EVMs as cross-referencing oracles avoiding manual checking. For evaluation, we selected four widely used EVMs for the test, conducted large-scale mutation on 36,295 real-world smart contracts, and generated 253,153 smart contracts as initial seeds. Accompanied by manual root cause analysis, we found five previously unknown security bugs and all had been included in the common vulnerabilities and exposures (CVE) database.},
  year     = {2024}
}

@inproceedings{NEZHA,
  author    = {Petsios, Theofilos and Tang, Adrian and Stolfo, Salvatore and Keromytis, Angelos D. and Jana, Suman},
  booktitle = {2017 IEEE Symposium on Security and Privacy (SP)},
  title     = {{NEZHA}: Efficient Domain-Independent Differential Testing},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {615-632},
  keywords  = {Testing;Computer bugs;Semantics;Tools;Software;Libraries;Security;nezha;differential testing;fuzzing},
  doi       = {10.1109/SP.2017.27}
}


@inproceedings{TLS,
  author    = {Walz, Andreas and Sikora, Axel},
  booktitle = {2018 International Carnahan Conference on Security Technology (ICCST)},
  title     = {Maximizing and Leveraging Behavioral Discrepancies in TLS Implementations using Response-Guided Differential Fuzzing},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {1-5},
  keywords  = {Fuzzing;Protocols;Security;Servers;Indexes;Probability distribution;TLS;network security;protocol implementation security;fuzzing;differential testing},
  doi       = {10.1109/CCST.2018.8585565}
}

@article{ParDiff,
  author     = {Zheng, Mingwei and Shi, Qingkai and Liu, Xuwei and Xu, Xiangzhe and Yu, Le and Liu, Congyu and Wei, Guannan and Zhang, Xiangyu},
  title      = {{ParDiff}: Practical Static Differential Analysis of Network Protocol Parsers},
  year       = {2024},
  issue_date = {April 2024},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {8},
  number     = {OOPSLA1},
  url        = {https://doi.org/10.1145/3649854},
  doi        = {10.1145/3649854},
  abstract   = {Countless devices all over the world are connected by networks and communicated via network protocols. Just like common software, protocol implementations suffer from bugs, many of which only cause silent data corruption instead of crashes. Hence, existing automated bug-finding techniques focused on memory safety, such as fuzzing, can hardly detect them. In this work, we propose a static differential analysis called ParDiff to find protocol implementation bugs, especially silent ones hidden in message parsers. Our key observation is that a network protocol often has multiple implementations and any semantic discrepancy between them may indicate bugs. However, different implementations are often written in disparate styles, e.g., using different data structures or written with different control structures, making it challenging to directly compare two implementations of even the same protocol. To exploit this observation and effectively compare multiple protocol implementations, ParDiff (1) automatically extracts finite state machines from programs to represent protocol format specifications, and (2) then leverages bisimulation and SMT solvers to find fine-grained and  
                semantic inconsistencies between them. We have extensively evaluated ParDiff using 14 network protocols. The results show that ParDiff outperforms both differential symbolic execution and differential fuzzing tools. To date, we have detected 41 bugs with 25 confirmed by developers.},
  journal    = {Proc. ACM Program. Lang.},
  month      = {04},
  articleno  = {137},
  numpages   = {27},
  keywords   = {Network protocol, differential analysis, protocol format specification, static program analysis}
}

@inproceedings{NeoDiff,
  author    = {Maier, Dominik and F\"{a}\ss{}ler, Fabian and Seifert, Jean-Pierre},
  title     = {Uncovering Smart Contract VM Bugs Via Differential Fuzzing},
  year      = {2022},
  isbn      = {9781450396028},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3503921.3503923},
  doi       = {10.1145/3503921.3503923},
  booktitle = {Reversing and Offensive-Oriented Trends Symposium},
  pages     = {11–22},
  numpages  = {12},
  keywords  = {State-Aware, Smart Contract VM, Differential Fuzzing},
  location  = {Vienna, Austria},
  series    = {ROOTS'21}
}

@inproceedings{ResolFuzz,
  author    = {Bushart, Jonas and Rossow, Christian},
  editor    = {Tsudik, Gene and Conti, Mauro and Liang, Kaitai and Smaragdakis, Georgios},
  title     = {ResolFuzz: Differential Fuzzing of DNS Resolvers},
  booktitle = {Computer Security -- ESORICS 2023},
  year      = {2024},
  publisher = {Springer Nature Switzerland},
  address   = {Cham},
  pages     = {62--80},
  abstract  = {This paper identifies and analyzes vulnerabilities in the DNS infrastructure, with particular focus on recursive DNS resolvers. We aim to identify semantic bugs that could lead to incorrect resolver responses, introducing risks to the internet's critical infrastructure. To achieve this, we introduce ResolFuzz, a mutation-based fuzzer to search for semantic differences across DNS resolver implementations. ResolFuzz combines differential analysis with a rule-based mechanism to distinguish between benign differences and potential threats. We evaluate our prototype on seven resolvers and uncover multiple security vulnerabilities, including inaccuracies in resolver responses and possible amplification issues in PowerDNS Recursor's handling of DNAME Resource Records (RRs). Moreover, we demonstrate the potential for self-sustaining DoS attacks in resolved and trust-dns, further underlining the necessity of comprehensive DNS security. Through these contributions, our research underscores the potential of differential fuzzing in uncovering DNS vulnerabilities.},
  isbn      = {978-3-031-51476-0}
}

@mastersthesis{WASM,
  author = {Hamidy, Gilang},
  school = {Aalto University},
  title  = {Differential Fuzzing the WebAssembly},
  date   = {2020-08-18}
}

@inproceedings{UndefinedBehavior,
  author    = {Li, Shaohua and Su, Zhendong},
  title     = {Finding Unstable Code via Compiler-Driven Differential Testing},
  year      = {2023},
  isbn      = {9781450399180},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3582016.3582053},
  doi       = {10.1145/3582016.3582053},
  abstract  = {Unstable code refers to code that has inconsistent or unstable run-time semantics due to undefined behavior (UB) in the program. Compilers exploit UB by assuming that UB never occurs, which allows them to generate efficient but potentially semantically inconsistent binaries. Practitioners have put great research and engineering effort into designing dynamic tools such as sanitizers for frequently occurring UBs. However, it remains a big challenge how to detect UBs that are beyond the reach of current techniques. In this paper, we introduce compiler-driven differential testing (CompDiff), a simple yet effective approach for finding unstable code in C/C++ programs. CompDiff relies on the fact that when compiling unstable code, different compiler implementations may produce semantically inconsistent binaries. Our main approach is to examine the outputs of different binaries on the same input. Discrepancies in outputs may signify the existence of unstable code. To detect unstable code in real-world programs, we also integrate CompDiff into AFL++, the most widely-used and actively-maintained general-purpose fuzzer. Despite its simplicity, CompDiff is effective in practice: on the Juliet benchmark programs, CompDiff uniquely detected 1,409 bugs compared to sanitizers; on 23 popular open-source C/C++ projects, CompDiff-AFL++ uncovered 78 new bugs, 52 of which have been fixed by developers and 36 cannot be detected by sanitizers. Our evaluation also reveals the fact that CompDiff is not designed to replace current UB detectors but to complement them.},
  booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages     = {238–251},
  numpages  = {14},
  keywords  = {unstable code, undefined behavior, compiler},
  location  = {<conf-loc>, <city>Vancouver</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>},
  series    = {ASPLOS 2023}
}

@inproceedings{CompilerDL,
  author    = {Cummins, Chris and Petoumenos, Pavlos and Murray, Alastair and Leather, Hugh},
  title     = {Compiler fuzzing through deep learning},
  year      = {2018},
  isbn      = {9781450356992},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3213846.3213848},
  doi       = {10.1145/3213846.3213848},
  abstract  = {Random program generation — fuzzing — is an effective technique for discovering bugs in compilers but successful fuzzers require extensive development effort for every language supported by the compiler, and often leave parts of the language space untested. We introduce DeepSmith, a novel machine learning approach to accelerating compiler validation through the inference of generative models for compiler inputs. Our approach infers a learned model of the structure of real world code based on a large corpus of open source code. Then, it uses the model to automatically generate tens of thousands of realistic programs. Finally, we apply established differential testing methodologies on them to expose bugs in compilers. We apply our approach to the OpenCL programming language, automatically exposing bugs with little effort on our side. In 1,000 hours of automated testing of commercial and open source compilers, we discover bugs in all of them, submitting 67 bug reports. Our test cases are on average two orders of magnitude smaller than the state-of-the-art, require 3.03\texttimes{} less time to generate and evaluate, and expose bugs which the state-of-the-art cannot. Our random program generator, comprising only 500 lines of code, took 12 hours to train for OpenCL versus the state-of-the-art taking 9 man months to port from a generator for C and 50,000 lines of code. With 18 lines of code we extended our program generator to a second language, uncovering crashes in Solidity compilers in 12 hours of automated testing.},
  booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
  pages     = {95–105},
  numpages  = {11},
  keywords  = {Compiler Fuzzing, Deep Learning, Differential Testing},
  location  = {Amsterdam, Netherlands},
  series    = {ISSTA 2018}
}